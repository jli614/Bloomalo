{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jli614/Bloomalo/blob/main/Bloomalo_5916.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxY3PuqHtcxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd62f03f-30f1-41aa-9058-2a83e73f89f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.4)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.7)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.9.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting trafilatura\n",
            "  Downloading trafilatura-1.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (2.1.1)\n",
            "Requirement already satisfied: lxml>=4.6.4 in /usr/local/lib/python3.7/dist-packages (from trafilatura) (4.9.1)\n",
            "Collecting htmldate>=1.3.0\n",
            "  Downloading htmldate-1.3.1-py3-none-any.whl (39 kB)\n",
            "Collecting justext>=3.0.0\n",
            "  Downloading jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[K     |████████████████████████████████| 837 kB 57.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from trafilatura) (2022.9.24)\n",
            "Collecting courlan>=0.8.3\n",
            "  Downloading courlan-0.8.3-py3-none-any.whl (34 kB)\n",
            "Collecting urllib3<2,>=1.26\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 46.0 MB/s \n",
            "\u001b[?25hCollecting tld>=0.12.6\n",
            "  Downloading tld-0.12.6-py37-none-any.whl (412 kB)\n",
            "\u001b[K     |████████████████████████████████| 412 kB 45.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.7/dist-packages (from courlan>=0.8.3->trafilatura) (3.3.0)\n",
            "Collecting dateparser>=1.1.1\n",
            "  Downloading dateparser-1.1.1-py2.py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 31.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from htmldate>=1.3.0->trafilatura) (2.8.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.1.1->htmldate>=1.3.0->trafilatura) (1.5.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.1.1->htmldate>=1.3.0->trafilatura) (2022.4)\n",
            "Collecting regex!=2019.02.19,!=2021.8.27,<2022.3.15\n",
            "  Downloading regex-2022.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n",
            "\u001b[K     |████████████████████████████████| 749 kB 59.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.2->htmldate>=1.3.0->trafilatura) (1.15.0)\n",
            "Installing collected packages: regex, urllib3, tld, dateparser, justext, htmldate, courlan, trafilatura\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2022.6.2\n",
            "    Uninstalling regex-2022.6.2:\n",
            "      Successfully uninstalled regex-2022.6.2\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.12 which is incompatible.\u001b[0m\n",
            "Successfully installed courlan-0.8.3 dateparser-1.1.1 htmldate-1.3.1 justext-3.0.0 regex-2022.3.2 tld-0.12.6 trafilatura-1.3.0 urllib3-1.26.12\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4\n",
        "!pip install numpy\n",
        "!pip install requests\n",
        "!pip install spacy\n",
        "!pip install trafilatura\n",
        "!pip install nltk\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "from requests.models import MissingSchema\n",
        "import spacy\n",
        "import trafilatura\n",
        "import ssl\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ],
      "metadata": {
        "id": "nuaBCtE1yrbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50bd1ea4-2e92-41fb-b677-abd8f7c11ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "URL = [\"http://aap.org\", \"http://Brightfutures.aap.org\", \"https://www.mother.ly/\", \"http://cdc.gov\", \"http://Pediatrics.aappublications.org\", \"http://Psychologytoday.com\", \"http://www.parents.com\", \"http://www.atlantaparent.com\", \"http://www.parenting.com\", \"http://Kidshealth.org\", \"http://www.fatherly.com\", \"http://www.childrensmd.org\", \"https://www.kckidsdoc.com\", \"https://www.pediatricsnow.com\", \"https://www.drgreene.com\", \"https://blog.cincinnatichildrens.org\", \"https://www.today.com/parenting-guides\", \"https://www.babycenter.com\", \"https://www.more4kids.info\", \"https://childdevelopmentinfo.com/\", \"https://www.healthline.com/parenthood\",  \"https://www.psy-ed.com/wpblog/\", \"https://developingminds.net.au/blog/\" ]\n",
        "dictionary = {} # Was originally named dict but https://stackoverflow.com/questions/68691468/jupyter-is-showing-callable-error-in-dict-function?noredirect=1&lq=1\n",
        "for url in range(len(URL)):\n",
        "    req = requests.get(URL[url])\n",
        "   # print(URL[url])\n",
        "    soup = BeautifulSoup(req.text, 'html.parser')\n",
        "    text = soup.find_all(text=True)\n",
        "    output = ''\n",
        "    totalOutput = ''\n",
        "    blacklist = [\n",
        "      '[document]',\n",
        "\n",
        "      'noscript',\n",
        "        'header',\n",
        "        'html',\n",
        "        'meta',\n",
        "        'head', \n",
        "        'input',\n",
        "        'script',\n",
        "        'style'\n",
        "    # there may be more elements you don't want, such as \"style\", etc.\n",
        "    ]\n",
        "\n",
        "    for t in text:\n",
        "        if t.parent.name not in blacklist and t != '\\n':\n",
        "            output += '{} '.format(t)\n",
        "            totalOutput +=output\n",
        "    dictionary[URL[url]] = output.lower()\n",
        "#print(dictionary)"
      ],
      "metadata": {
        "id": "4CiQ1J1e3ljS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/56638590/how-to-implement-multi-keyword-search-in-c\n",
        "keyword = input(\"Search for a term: \").lower()\n",
        "keyword_list_non_stem_tokenize = word_tokenize(keyword)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "keyword_list_non_stem = [word for word in keyword_list_non_stem_tokenize if word not in stop_words]\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmer  = PorterStemmer()\n",
        "keyword_list_stem = []\n",
        "for word in keyword_list_non_stem:\n",
        "  keyword_list_stem.append(porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word)))\n",
        "print(keyword_list_stem)"
      ],
      "metadata": {
        "id": "0-MnDNuw9MLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#url names are key and stemmed lines are values\n",
        "keyword_websites_stemmed = {}\n",
        "\n",
        "#gets each key, value pair in dictionary holding url as key and full website text as value \n",
        "for key, value in dictionary.items():\n",
        "\n",
        "  #get array of each line in the text of the website\n",
        "  valueList = value.splitlines()\n",
        "  for line in valueList:\n",
        "    #print(line)\n",
        "    for keyword in keyword_list_stem:\n",
        "\n",
        "      #if keyword is found in line\n",
        "      if line.find(keyword) != -1:\n",
        "        if key not in keyword_websites_stemmed:\n",
        "\n",
        "          #gets stem of each word in text and adds to dictionary\n",
        "          tokenized_values = word_tokenize(value)\n",
        "\n",
        "          #print(\"Tokenized values: \" + tokenized_values[0])\n",
        "          stemmed_values = []\n",
        "          for word in tokenized_values:\n",
        "            stemmed_values.append(porter_stemmer.stem(word))\n",
        "          #print(\"Stemmed values: \" + stemmed_values[0])\n",
        "          joined_values = ' '.join(stemmed_values)\n",
        "          keyword_websites_stemmed.update({key: joined_values})\n",
        "          \n",
        "        print(joined_values)\n",
        "        print(\"On website: \" + key)\n",
        "        print(joined_values)\n",
        "        print(\"Keyword is: \" + keyword)\n",
        "    #print(keyword_websites.get('http://aap.org'))"
      ],
      "metadata": {
        "id": "d31sTMRR_PXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(list1, list2):\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
        "    return float(intersection) / union"
      ],
      "metadata": {
        "id": "MSwtOEDsTRTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/4326658/how-to-index-into-a-dictionary\n",
        "first_key = list(keyword_websites_stemmed)[0]\n",
        "first_val = list(keyword_websites_stemmed.values())[0]\n",
        "second_val = list(keyword_websites_stemmed.values())[2]\n",
        "#print(first_key.count)\n",
        "print(len(keyword_websites_stemmed))\n",
        "#x = keyword_websites_stemmed.popitem()\n",
        "#print(x)"
      ],
      "metadata": {
        "id": "TQJ-qAUwUsmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the desired examination for Jaccard similiary"
      ],
      "metadata": {
        "id": "YTzEuXno8FjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l =  list(keyword_websites_stemmed.values())\n",
        "l_keys = list(keyword_websites_stemmed.keys())\n",
        "jac_sim_list = []\n",
        "print(l_keys)\n",
        "length = len(l)\n",
        "for i in range(length):\n",
        "  x = l[i]\n",
        "  url_similarity_l = []\n",
        "  for j in range(length):\n",
        "    y = l[j]\n",
        "    output = jaccard_similarity(x, y)\n",
        "    url_similarity_l.append(output)\n",
        "  jac_sim_list.append(url_similarity_l)\n",
        "df = pd.DataFrame(jac_sim_list, index=l_keys, columns= l_keys)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "fRa61ANv6lmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(list1, list2):\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(set(list1)) + len(set(list2))) - intersection\n",
        "    return float(intersection) / union"
      ],
      "metadata": {
        "id": "IYc1gyjo29J2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import jaccard_distance\n",
        "f = jaccard_similarity(first_val, second_val)\n",
        "print(f)"
      ],
      "metadata": {
        "id": "PtBtbHWl3JL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/37003272/how-to-compute-jaccard-similarity-from-a-pandas-dataframe\n",
        "#https://stackoverflow.com/questions/37003272/how-to-compute-jaccard-similarity-from-a-pandas-dataframe  <- shows to make a grid using Dataframes..."
      ],
      "metadata": {
        "id": "A2gA8qR_3-CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/27488446/how-do-i-get-word-frequency-in-a-corpus-using-scikit-learn-countvectorizer?noredirect=1&lq=1\n",
        "\n",
        "\n",
        "texts = l\n",
        "cv = CountVectorizer()\n",
        "cv_fit = cv.fit_transform(texts)\n",
        "\n",
        "print(cv.get_feature_names())\n",
        "print(cv_fit.toarray())\n",
        "print(np.asarray(cv_fit.sum(axis=0)))\n",
        "# [2 3 2 2]\n",
        "\n",
        "for key, value in dictionary.items():\n",
        "    text+= value\n",
        "\n",
        "raw = ' '.join(word_tokenize(totalOutput)) #totalOutput is from earlier\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'[A-Za-z]{2,}')\n",
        "words = tokenizer.tokenize(raw)\n",
        "\n",
        "# remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# count word frequency, sort and return just 20\n",
        "counter = Counter()\n",
        "counter.update(words)\n",
        "most_common = counter.most_common(20)\n",
        "most_common"
      ],
      "metadata": {
        "id": "G55ATR1QDWe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZRN6Ue9eJuzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a"
      ],
      "metadata": {
        "id": "rviVi2M9HQeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 3 words for each query -> Get a TF-IDF between the query and the document (there may be better ways)\n",
        "TF-IDF among the documents, not necessarily the documents and the query can be an indication of diversity\n",
        "Use Euclidean Distance to help determine relevance criterion\n",
        "Can use readibility function in some Python toolkit\n",
        "Convert a document into a numeric representation\n",
        "\n",
        "TF-IDF does have limitations\n",
        "\n",
        "Recall and precision for quality???\n",
        "LREC and CREC from Mit (Government agency) could be tools to look at tools.\n",
        "\n",
        "\n",
        "Create your own small dataset\n",
        "\n",
        "Concept of Elastic Search\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "lDNC-zaiDU7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/27488446/how-do-i-get-word-frequency-in-a-corpus-using-scikit-learn-countvectorizer?noredirect=1&lq=1\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texts = l\n",
        "cv = CountVectorizer()\n",
        "cv_fit = cv.fit_transform(texts)\n",
        "\n",
        "print(cv.get_feature_names())\n",
        "print(cv_fit.toarray())\n",
        "print(np.asarray(cv_fit.sum(axis=0)))\n",
        "# [2 3 2 2]\n",
        "\n",
        "for key, value in dictionary.items():\n",
        "    text+= value\n",
        "\n",
        "raw = ' '.join(word_tokenize(totalOutput)) #totalOutput is from earlier\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'[A-Za-z]{2,}')\n",
        "words = tokenizer.tokenize(raw)\n",
        "\n",
        "# remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# count word frequency, sort and return just 20\n",
        "counter = Counter()\n",
        "counter.update(words)\n",
        "most_common = counter.most_common(20)\n",
        "most_common"
      ],
      "metadata": {
        "id": "2M-XmUXMIZW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.codingame.com/playgrounds/6233/what-is-idf-and-how-is-it-calculated"
      ],
      "metadata": {
        "id": "04X8lcRxJYJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features( document ):\n",
        "   terms = tuple(document.lower().split())\n",
        "   features = set()\n",
        "   for i in range(len(terms)):\n",
        "      for n in range(1,4):\n",
        "          if i+n <= len(terms):\n",
        "              features.add(terms[i:i+n])\n",
        "   return features\n",
        "\n",
        "\n",
        "\n",
        "documents = l\n",
        "\n",
        "def calculate_idf( documents ):\n",
        "   N = len(documents)\n",
        "   from collections import Counter\n",
        "   tD = Counter()\n",
        "   for d in documents:\n",
        "     \n",
        "      features = extract_features(d)\n",
        "      for f in features:\n",
        "          tD[\" \".join(f)] += 1\n",
        "   IDF = []\n",
        "   import math\n",
        "   for (term,term_frequency) in tD.items():\n",
        "       term_IDF = math.log(float(N) / term_frequency)\n",
        "       IDF.append(( term_IDF, term ))\n",
        "   IDF.sort(reverse=True)\n",
        "   return IDF\n",
        "\n",
        "#for (IDF, term) in calculate_idf(documents):\n",
        "    #print(IDF, term)\n"
      ],
      "metadata": {
        "id": "UN2Zt7sNJuMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import builtins\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(documents)\n",
        "print(vectorizer.get_feature_names())\n",
        "#print(X.shape)\n",
        "print(X.toarray()[0])\n",
        "sharp = X.toarray()[0]\n",
        "dominos = vectorizer.get_feature_names()\n",
        "print(dict(zip(dominos, sharp)))\n",
        "print()\n",
        "dictA = {'a':5,'c':5}\n",
        "#new_dic = dict()\n",
        "#print(dict(zip(vectorizer.get_feature_names(), X.toarray()[0])))"
      ],
      "metadata": {
        "id": "f1n_jjo0Lfj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://stackoverflow.com/questions/61168210/is-there-any-way-to-use-tkinter-with-google-colaboratory"
      ],
      "metadata": {
        "id": "86yKbnjJfAbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://goodboychan.github.io/python/datacamp/natural_language_processing/2020/07/17/04-TF-IDF-and-similarity-scores.html#Cosine-similarity\n",
        "#https://github.com/asvskartheek/Text-Retrieval/blob/master/TF-IDF%20Search%20Engine%20(SKLEARN).ipynb\n",
        "\n",
        "#takes a query, finds cosine similarity between query and rest of websites\n",
        "#prints top 10 most similar websites\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "pd_tfid = pd.DataFrame(list(keyword_websites_stemmed.items()), columns=['url', 'text'])\n",
        "#print(pd_tfid)\n",
        "X = vectorizer.fit_transform(pd_tfid['text'])\n",
        "#print(X.shape)\n",
        "joined_keyword_list_stemmed = ' '.join(keyword_list_stem)\n",
        "query_vec = vectorizer.transform([joined_keyword_list_stemmed]) \n",
        "results = cosine_similarity(X,query_vec).reshape((-1,)) \n",
        "for i in results.argsort()[-10:][::-1]:\n",
        "    print(pd_tfid.iloc[i,0],\"--\",pd_tfid.iloc[i,1])\n"
      ],
      "metadata": {
        "id": "0cipFjW71l6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_url(joined_keyword_list_stemmed, websites):\n",
        "  "
      ],
      "metadata": {
        "id": "ngC9yFKJAzZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indices = pd.Series(pd_tfid.index, index=pd_tfid['url']).drop_duplicates()\n",
        "\n",
        "def get_recommendations(url, cosine_sim, indices):\n",
        "    idx = indices[url]\n",
        "    # Get the pairwsie similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    # Sort the urls based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=False)\n",
        "    # Get the scores for 10 most similar urls\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the indices\n",
        "    url_indices = [i[0] for i in sim_scores]\n",
        "    # Return the top 10 most similar websites\n",
        "    return pd_tfid['url'].iloc[url_indices]"
      ],
      "metadata": {
        "id": "9bxwAgEf6_3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_recommendations(\"http://Kidshealth.org\", results, indices))"
      ],
      "metadata": {
        "id": "U306hiQMCCc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install py-readability-metrics\n",
        "from readability import Readability\n",
        "#from readability import Readability"
      ],
      "metadata": {
        "id": "-v-XL2QgvPuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in dictionary.items():\n",
        "  if not len(value) < 100:\n",
        "    r = Readability(value)\n",
        "    \n",
        "    ari = r.ari()\n",
        "    print(key)\n",
        "    print(ari.score)\n",
        "    print(ari.grade_levels)\n",
        "    print(ari.ages)"
      ],
      "metadata": {
        "id": "tDAc-oCJxcgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk\n",
        "from tkinter import messagebox as mb\n",
        "\n",
        "def answer():\n",
        "    mb.showerror(\"Answer\", \"Sorry, no answer available\")\n",
        "\n",
        "def callback():\n",
        "    if mb.askyesno('Verify', 'Really quit?'):\n",
        "        mb.showwarning('Yes', 'Not yet implemented')\n",
        "    else:\n",
        "        mb.showinfo('No', 'Quit has been cancelled')\n",
        "\n",
        "tk.Button(text='Quit', command=callback).pack(fill=tk.X)\n",
        "tk.Button(text='Answer', command=answer).pack(fill=tk.X)\n",
        "tk.mainloop()"
      ],
      "metadata": {
        "id": "3wa1RUrX9z_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://medium.com/mlearning-ai/text-clustering-with-tf-idf-in-python-c94cd26a31e7\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# initialize kmeans with 3 centroids\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "# fit the model\n",
        "kmeans.fit(X)\n",
        "# store cluster labels in a variable\n",
        "clusters = kmeans.labels_"
      ],
      "metadata": {
        "id": "WNTGGurZhiYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# initialize PCA with 2 components\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "# pass our X to the pca and store the reduced vectors into pca_vecs\n",
        "pca_vecs = pca.fit_transform(X.toarray())\n",
        "# save our two dimensions into x0 and x1\n",
        "x0 = pca_vecs[:, 0]\n",
        "x1 = pca_vecs[:, 1]"
      ],
      "metadata": {
        "id": "iSqMNs7niGUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sUyCnebGJBXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a diagnostic tool..."
      ],
      "metadata": {
        "id": "q_SR5ekzJB50"
      }
    }
  ]
}